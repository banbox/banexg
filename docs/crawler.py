import asyncio
import re
import os
from pathlib import Path
from urllib.parse import urljoin, urlparse
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode


# --- æ ¸å¿ƒé…ç½®åŒº ---
START_URL = "https://bybit-exchange.github.io/docs/v5/guide"      # èµ·å§‹ URL
URL_REGEX = r".*/docs/v5/.*"           # åªçˆ¬å–åŒ…å«ç¬¦åˆç‰¹å®šæ­£åˆ™çš„é“¾æ¥
MAX_PAGES = 1000
OUTPUT_BASE_DIR = "bybit_docs"
CONCURRENT_COUNT = 5 

# --- ç½‘é¡µå†…å®¹è§£æé…ç½® (åœ¨è¿™é‡Œå±è”½èœå•å’Œä¾§è¾¹æ ) ---
# 1. æ’é™¤æ¨¡å¼ï¼šåˆ—å‡ºæ‰€æœ‰ä½ ä¸æƒ³çœ‹åˆ°çš„ HTML æ ‡ç­¾ã€Class æˆ– ID
EXCLUDED_TAGS = [
    "nav", 
    "footer", 
    "header", 
    "aside", 
]

# 2. èšç„¦æ¨¡å¼ï¼šå¦‚æœä½ åªæƒ³æŠ“å–æŸä¸ªç‰¹å®šåŒºåŸŸï¼Œå¡«å†™å®ƒçš„ CSS é€‰æ‹©å™¨ã€‚
# ä¾‹å¦‚ "main" æˆ– "#content"ã€‚å¦‚æœè®¾ä¸º Noneï¼Œåˆ™æŠ“å–é™¤ä¸Šè¿°æ’é™¤é¡¹ä¹‹å¤–çš„æ•´é¡µã€‚
CONTENT_SELECTORS = ['article']
# ------------------


def url_to_filepath(url, base_dir):
    """å°† URL æ˜ å°„ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
    parsed = urlparse(normalize_url(url))
    path_str = parsed.path.strip("/")
    path_parts = [p for p in path_str.split('/') if p]
    if not path_parts:
        path_parts = ["index"]
    
    full_path = Path(base_dir) / parsed.netloc / Path(*path_parts)
    return full_path.with_suffix(".md")

def normalize_url(url):
    """ç»Ÿä¸€è§„èŒƒ URLï¼šç§»é™¤ query/fragmentï¼Œå¹¶å»æ‰æœ«å°¾æ–œæ """
    parsed = urlparse(url)
    normalized = parsed._replace(query="", fragment="", params="").geturl()
    return normalized.rstrip("/")

async def save_to_markdown(result, base_dir):
    """ä¿å­˜çˆ¬å–åˆ°çš„å†…å®¹"""
    if not result.success or not result.markdown:
        return False

    file_path = url_to_filepath(result.url, base_dir)
    file_path.parent.mkdir(parents=True, exist_ok=True)
    
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(result.markdown)
        return True
    except Exception as e:
        print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ {result.url}: {e}")
        return False

async def exhaustive_crawl():
    visited = set()            
    to_visit = {START_URL}     
    processing = set()         
    
    regex = re.compile(URL_REGEX)
    
    # æ„é€ çˆ¬è™«é…ç½®
    config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        exclude_external_links=True,
        # åº”ç”¨å…¨å±€é…ç½®
        target_elements=CONTENT_SELECTORS,
        excluded_tags=EXCLUDED_TAGS,
        # è‡ªåŠ¨ç§»é™¤å¸¸è§çš„é®ç½©å±‚å’Œå¼¹çª—
        remove_overlay_elements=True
    )

    Path(OUTPUT_BASE_DIR).mkdir(parents=True, exist_ok=True)

    async with AsyncWebCrawler() as crawler:
        while (to_visit or processing) and len(visited) < MAX_PAGES:
            current_batch = []
            while to_visit and len(current_batch) < CONCURRENT_COUNT:
                url = to_visit.pop()
                normalized_url = normalize_url(url)
                
                if normalized_url in visited or normalized_url in processing:
                    continue

                target_file = url_to_filepath(normalized_url, OUTPUT_BASE_DIR)
                if target_file.exists():
                    print(f"â­ï¸  è·³è¿‡ (å·²å­˜åœ¨): {normalized_url}")
                    visited.add(normalized_url)
                    continue

                current_batch.append(normalized_url)
                processing.add(normalized_url)
            
            if not current_batch:
                if not processing: break 
                await asyncio.sleep(0.5)
                continue

            print(f"ğŸŒ æ­£åœ¨çˆ¬å–æ–°é¡µé¢: {len(current_batch)} æ¡...")
            
            results = await crawler.arun_many(current_batch, config=config)
            
            for result in results:
                curr_url = normalize_url(result.url)
                if curr_url in processing:
                    processing.remove(curr_url)
                
                if not result.success:
                    continue
                
                visited.add(curr_url)
                print(f"âœ… æˆåŠŸä¸‹è½½: {curr_url}")
                
                await save_to_markdown(result, OUTPUT_BASE_DIR)

                links = result.links or {}
                internal_links = links.get("internal", [])
                for link in internal_links:
                    href = link.get("href")
                    if not href: continue
                    
                    full_url = normalize_url(urljoin(result.url, href))
                    
                    if regex.search(full_url) and full_url not in visited and full_url not in processing:
                        to_visit.add(full_url)

    print(f"\nâœ¨ ä»»åŠ¡å®Œæˆã€‚å½“å‰æœ¬åœ°åº“å…±è®¡: {len(visited)} ä¸ªé¡µé¢ã€‚")

if __name__ == "__main__":
    asyncio.run(exhaustive_crawl())
